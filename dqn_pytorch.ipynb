{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym_waf\n",
    "RL模型抽象:  \n",
    "    action: 混淆操作，能够对xss进行混淆的操作，这里包括了  \n",
    "    state: 这里使用xss攻击字符串中的各个字符对应的ascii个数来与xss攻击字符长度来进行刻画，257维向量  \n",
    "    reward: 成功绕过检测模型则认为模型绕过成功了  \n",
    "\n",
    "\n",
    "\n",
    "Rl进行Xss绕过的限比较多，包括：\n",
    "- 这里的state表示其实存在比较大的问题，xss攻击字符串中的个字符对应的ascii与xss攻击字符串长度虽然能够是直接收到action影响，并且也是直接影响是否能够xss攻击成功，但是这种刻画方式一方面过于简单了，并不能对各个字符在攻击payload中的位置与是否组成了单词进行刻画，而且这种采用xss payload自身相关的payload，并没有考虑到攻击系统的环境问题，因此在不同的防御环境中并不会指定专业的策略来选择action\n",
    "- 模型的绕过能力很大程度上受限于提供的action，即可供选择的混淆操作\n",
    "- 模型的作用是能够再更少的尝试次数下生成对抗样本，主要作用是减少了尝试次数，并不是做了一些传统方法做不了的工作，只是减少尝试次数而已"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch                                    # 导入torch\n",
    "import torch.nn as nn                           # 导入torch.nn\n",
    "import torch.nn.functional as F                 # 导入torch.nn.functional\n",
    "import numpy as np                              # 导入numpy\n",
    "\n",
    "import gym_waf.envs.wafEnv\n",
    "from gym_waf.envs.wafEnv  import samples_test, samples_train\n",
    "from gym_waf.envs.features import Features\n",
    "from gym_waf.envs.waf import Waf_Check\n",
    "from gym_waf.envs.xss_manipulator import Xss_Manipulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'Waf-v0' # 之前注册的环境名，要按这样的形式命名，否则报错\n",
    "env = gym.make(ENV_NAME)                        # 使用gym库中的环境：CartPole，且打开封装(若想了解该环境，请自行百度)\n",
    "\n",
    "\n",
    "# 超参数\n",
    "BATCH_SIZE = 32                                 # 样本数量\n",
    "LR = 0.01                                       # 学习率\n",
    "EPSILON = 0.9                                   # greedy policy\n",
    "GAMMA = 0.9                                     # reward discount\n",
    "TARGET_REPLACE_ITER = 100                       # 目标网络更新频率\n",
    "MEMORY_CAPACITY = 2000                          # 记忆库容量\n",
    "N_ACTIONS = env.action_space.n                  \n",
    "STATES_DIM = env.observation_space.shape[1]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.DQN import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dqn,episode,max_episode_steps,memory_capacity):\n",
    "    \"\"\"\n",
    "    模型训练\n",
    "        param dqn:要训练的DQN模型\n",
    "        param episode: 训练样本抽样的进行强化学习的次数\n",
    "        param max_episode_steps: 一个episode中最多能进行action次数\n",
    "        param memory_capacity: 记忆库容量\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(episode):                                                # 使用的训练样本数\n",
    "        s = env.reset()                                                     # 重置环境\n",
    "        episode_reward_sum = 0                                              # 初始化该循环对应的episode的总奖励\n",
    "        step = 0\n",
    "\n",
    "        done = False\n",
    "        # 样本成功完成任务或到达可走最大step结束循环\n",
    "        while not done or step<max_episode_steps:                                                \n",
    "            step += 1\n",
    "            a = dqn.choose_action(s)                                        # 输入该步对应的状态s，选择动作\n",
    "            s_, r, done, info = env.step(a)                                 # 执行动作，获得反馈\n",
    "            dqn.store_transition(s, a, r, s_)                               # 存储样本\n",
    "            episode_reward_sum += r                                         # 逐步加上一个episode内每个step的reward\n",
    "\n",
    "            s = s_                                                          # 更新状态\n",
    "\n",
    "            if dqn.memory_counter > memory_capacity:                        \n",
    "                dqn.learn()   \n",
    "    return dqn\n",
    "\n",
    "def test(dqn,test_data,max_episode_steps,action_lookup):\n",
    "    \"\"\"\n",
    "    测试模型效果\n",
    "    param dqn: 要进行测试的DQN模型\n",
    "    param test_data: 测试数据集\n",
    "    param max_episode_steps: 一个episode中最多能进行action的次数\n",
    "    param action_lookup: 行为值到具体操作字符串的映射\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    features_extra = Features()     # 特征向量\n",
    "    waf_checker = Waf_Check()       # waf检验免杀效果\n",
    "    xss_manipulatorer = Xss_Manipulator()   # 根据动作修改当前样本，来达到免杀\n",
    "    \n",
    "    success = 0     # 免杀成功数\n",
    "    sum = 0         # 总数目\n",
    "    \n",
    "    for sample in test_data:\n",
    "        sum += 1\n",
    "\n",
    "        for _ in range(max_episode_steps):\n",
    "            if not waf_checker.check_xss(sample) :\n",
    "                success += 1\n",
    "                break\n",
    "            f = features_extra.extract(sample)\n",
    "            f = torch.FloatTensor(f)                                    # 将x转换成32-bit floating point形式，并在dim=0增加维数为1的维度\n",
    "            actions_value = dqn.eval_net.forward(f)                            # 通过对评估网络输入状态x，前向传播获得动作值\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()                # 输出每一行最大值的索引，并转化为numpy ndarray形式\n",
    "            action = action[0]    \n",
    "            sample = xss_manipulatorer.modify(sample,action_lookup[action])\n",
    "    \n",
    "    print(\"总数量：{} 成功：{}\".format(sum,success))\n",
    "    return sum,success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造动作速查表\n",
    "ACTION_LOOKUP = {i:act for i,act in enumerate(Xss_Manipulator.ACTION_TABLE.keys())} # key为原动作字典的下标0123，value为原动作字典的key即免杀操作名\n",
    "\n",
    "\n",
    "max_episode_steps = 5\n",
    "\n",
    "epoch = 5\n",
    "episode = 10000\n",
    "success_sum = 0\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    dqn = DQN(STATES_DIM,N_ACTIONS,MEMORY_CAPACITY,EPSILON,GAMMA,TARGET_REPLACE_ITER,BATCH_SIZE,LR)\n",
    "    dqn = train(dqn,episode,max_episode_steps,MEMORY_CAPACITY)\n",
    "    sum,success = test(dqn,samples_test,max_episode_steps,ACTION_LOOKUP)\n",
    "    success_sum += success\n",
    "\n",
    "print(\"平均成功比例：{}/{}\".format(success_sum/epoch,sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数量：51 成功：35\n",
      "总数量：51 成功：35\n",
      "总数量：51 成功：36\n",
      "总数量：51 成功：40\n",
      "总数量：51 成功：29\n",
      "平均成功比例：35.0/51\n"
     ]
    }
   ],
   "source": [
    "# 构造动作速查表\n",
    "ACTION_LOOKUP = {i:act for i,act in enumerate(Xss_Manipulator.ACTION_TABLE.keys())} # key为原动作字典的下标0123，value为原动作字典的key即免杀操作名\n",
    "\n",
    "\n",
    "max_episode_steps = 5\n",
    "\n",
    "epoch = 5\n",
    "episode = 1\n",
    "success_sum = 0\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    dqn = DQN(STATES_DIM,N_ACTIONS,MEMORY_CAPACITY,EPSILON,GAMMA,TARGET_REPLACE_ITER,BATCH_SIZE,LR)\n",
    "    dqn = train(dqn,episode,max_episode_steps,MEMORY_CAPACITY)\n",
    "    sum,success = test(dqn,samples_test,max_episode_steps,ACTION_LOOKUP)\n",
    "    success_sum += success\n",
    "\n",
    "print(\"平均成功比例：{}/{}\".format(success_sum/epoch,sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总数量：51 成功：35\n",
      "总数量：51 成功：35\n",
      "总数量：51 成功：32\n",
      "总数量：51 成功：33\n",
      "总数量：51 成功：40\n",
      "平均成功比例：35.0/51\n"
     ]
    }
   ],
   "source": [
    "# 构造动作速查表\n",
    "ACTION_LOOKUP = {i:act for i,act in enumerate(Xss_Manipulator.ACTION_TABLE.keys())} # key为原动作字典的下标0123，value为原动作字典的key即免杀操作名\n",
    "\n",
    "\n",
    "max_episode_steps = 5\n",
    "\n",
    "epoch = 5\n",
    "episode = 1000\n",
    "success_sum = 0\n",
    "\n",
    "\n",
    "for i in range(epoch):\n",
    "    dqn = DQN(STATES_DIM,N_ACTIONS,MEMORY_CAPACITY,EPSILON,GAMMA,TARGET_REPLACE_ITER,BATCH_SIZE,LR)\n",
    "    dqn = train(dqn,episode,max_episode_steps,MEMORY_CAPACITY)\n",
    "    sum,success = test(dqn,samples_test,max_episode_steps,ACTION_LOOKUP)\n",
    "    success_sum += success\n",
    "\n",
    "print(\"平均成功比例：{}/{}\".format(success_sum/epoch,sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
