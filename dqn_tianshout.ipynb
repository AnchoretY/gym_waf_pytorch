{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tianshou as ts\n",
    "import numpy as np\n",
    "import torch                                    # 导入torch\n",
    "import torch.nn as nn                           # 导入torch.nn\n",
    "import torch.nn.functional as F                 # 导入torch.nn.functional\n",
    "\n",
    "import gym_waf.envs.wafEnv\n",
    "from gym_waf.envs.wafEnv  import samples_test, samples_train\n",
    "from gym_waf.envs.features import Features\n",
    "from gym_waf.envs.waf import Waf_Check\n",
    "from gym_waf.envs.xss_manipulator import Xss_Manipulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(*[\n",
    "            nn.Linear(np.prod(state_shape), 16), nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 32), nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16), nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, np.prod(action_shape))\n",
    "        ])\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        if not isinstance(obs, torch.Tensor):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        batch = obs.shape[0]\n",
    "        logits = self.model(obs.view(batch, -1))\n",
    "        return logits, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_shape:(1, 257)\n",
      "action_shape:4\n"
     ]
    }
   ],
   "source": [
    "# 创建虚拟环境\n",
    "ENV_NAME = 'Waf-v0' \n",
    "train_envs = gym.make(ENV_NAME)   \n",
    "test_envs = gym.make(ENV_NAME) \n",
    "\n",
    "state_shape = train_envs.observation_space.shape or train_envs.observation_space.n\n",
    "action_shape = test_envs.action_space.shape or test_envs.action_space.n\n",
    "print(\"state_shape:{}\".format(state_shape))\n",
    "print(\"action_shape:{}\".format(action_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "BATCH_SIZE = 32                                 # 样本数量\n",
    "LR = 0.01                                       # 学习率\n",
    "EPSILON = 0.9                                   # greedy policy\n",
    "GAMMA = 0.9                                     # reward discount\n",
    "TARGET_REPLACE_ITER = 100                       # 目标网络更新频率\n",
    "MEMORY_CAPACITY = 2000                          # 记忆库容量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试的最大次数\n",
    "max_episode_steps = 5     # fit训练时用到，在一次学习周期中的最大步数(默认一直学习直到“死”)\n",
    "\n",
    "# 构造动作速查表\n",
    "ACTION_LOOKUP = {i:act for i,act in enumerate(Xss_Manipulator.ACTION_TABLE.keys())} # key为原动作字典的下标0123，value为原动作字典的key即免杀操作名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "免杀操作的个数：\n",
      "(1, 257)\n",
      "观测值空间形状：\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [02:55, 56.99it/s, env_step=10000, len=4, loss=7.797, n/ep=1, n/st=1, rew=0.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 5.300000 ± 4.990992, best_reward: 5.700000 ± 4.950758 in #0\n",
      "总数量：51 成功：28\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4782ad88d150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0magent1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'waf-v0.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def train_dqn_model(layers, rounds=10000):\n",
    "    env = gym.make(ENV_NAME)    # 创建环境\n",
    "    env.seed(1)\n",
    "    window_length = 1       # 窗口长度，后面创建记忆体时用，通常设置为1\n",
    "\n",
    "    # 打印动作、观测值相关信息\n",
    "    print(\"免杀操作的个数：\")\n",
    "    print(state_shape)   # 免杀操作的个数，可自行增加\n",
    "    print(\"观测值空间形状：\")\n",
    "    print(action_shape)      # 为(1,257)\n",
    "\n",
    "    # 创建神经网络模型\n",
    "    model = Net(state_shape,action_shape)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    # 声明训练策略\n",
    "    policy = ts.policy.DQNPolicy(\n",
    "        model,\n",
    "        optim, \n",
    "        discount_factor=GAMMA,                                 # 奖励衰减率\n",
    "        estimation_step=window_length,                         # 窗口长度，后面创建记忆体时用，通常设置为1\n",
    "        target_update_freq=TARGET_REPLACE_ITER,                # target网络更新频率\n",
    "    )\n",
    "    \n",
    "    # 声明与环境直接进行交互的collectoer\n",
    "    train_collector = ts.data.Collector(policy, train_envs, ts.data.ReplayBuffer(size=MEMORY_CAPACITY))\n",
    "    test_collector = ts.data.Collector(policy, test_envs)\n",
    "    \n",
    "    # 训练\n",
    "    result = ts.trainer.offpolicy_trainer(\n",
    "        policy, train_collector, test_collector,\n",
    "        max_epoch=1,                                         # 训练的最大轮数\n",
    "        step_per_epoch=10000,                                    # 每轮训练要使用多少个随机样本进行强化学习\n",
    "        step_per_collect=1,                                    # 每收集多少个样本更新到eval网络一次\n",
    "        episode_per_test=100,                                  # 随机训练多少个样本以后在测试集上进行效果测试\n",
    "        batch_size=BATCH_SIZE,                                 \n",
    "        train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
    "        test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
    "        stop_fn = None \n",
    "#         stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold\n",
    "        )\n",
    "    \n",
    "    \n",
    "    features_extra = Features()     # 特征向量\n",
    "    waf_checker = Waf_Check()   # waf检验免杀效果\n",
    "    xss_manipulatorer = Xss_Manipulator()   # 根据动作修改当前样本，来达到免杀\n",
    "\n",
    "    success = 0     # 免杀成功数\n",
    "    sum = 0     # 总数目\n",
    "\n",
    "\n",
    "    for sample in samples_test:\n",
    "        sum += 1\n",
    "\n",
    "        for _ in range(max_episode_steps):\n",
    "            if not waf_checker.check_xss(sample) :\n",
    "                success += 1\n",
    "                break\n",
    "\n",
    "            #f = features_extra.extract(sample).reshape(shp)\n",
    "            f = features_extra.extract(sample)\n",
    "            act_values = model(f)\n",
    "            action = np.argmax(act_values[0].detach().numpy())\n",
    "            sample = xss_manipulatorer.modify(sample,ACTION_LOOKUP[action])\n",
    "\n",
    "    print(\"总数量：{} 成功：{}\".format(sum,success))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent1, model1 = train_dqn_model([5], rounds=100)\n",
    "    model1.save('waf-v0.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rl(features_extra = Features()     # 特征向量\n",
    "    waf_checker = Waf_Check()   # waf检验免杀效果\n",
    "    xss_manipulatorer = Xss_Manipulator()   # 根据动作修改当前样本，来达到免杀):\n",
    "            \n",
    "    success = 0     # 免杀成功数\n",
    "    sum = 0     # 总数目\n",
    "            \n",
    "    for sample in samples_test:\n",
    "        sum += 1\n",
    "\n",
    "        for _ in range(max_episode_steps):\n",
    "            if not waf_checker.check_xss(sample) :\n",
    "                success += 1\n",
    "                break\n",
    "\n",
    "            f = features_extra.extract(sample)\n",
    "            act_values = model(f)\n",
    "            action = np.argmax(act_values[0].detach().numpy())\n",
    "            sample = xss_manipulatorer.modify(sample,ACTION_LOOKUP[action])\n",
    "\n",
    "    print(\"总数量：{} 成功：{}\".format(sum,success))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
